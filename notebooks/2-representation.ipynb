{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pd.read_csv('/Users/blairjdaniel/lighthouse/lighthouse/LLM-Sentiment-Analysis-lhl/csv_files/train.csv')\n",
    "ds_test = pd.read_csv('/Users/blairjdaniel/lighthouse/lighthouse/LLM-Sentiment-Analysis-lhl/csv_files/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation from the text col\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Run the function on the datasets\n",
    "ds_train['text_tokenized'] = ds_train['text_tokenized'].apply(lambda x: remove_punctuation(x))\n",
    "ds_test['text_tokenized'] = ds_test['text_tokenized'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Clean and join the tokens in 'text_tokenized' into a single string for each document\n",
    "# ds_train = ds_train['text_tokenized'].apply(\n",
    "#     lambda tokens: \" \".join(token.strip() for token in tokens if token not in string.punctuation)\n",
    "# )\n",
    "# ds_test = ds_test['text_tokenized'].apply(\n",
    "#     lambda tokens: \" \".join(token.strip() for token in tokens if token not in string.punctuation)\n",
    "# )\n",
    "\n",
    "# # Check a few rows to verify the fix\n",
    "# print(ds_train.head())\n",
    "# print(ds_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PorterStemmer and WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i rent i am curiousyellow from my video store ...\n",
      "1    i am curiou yellow is a risibl and pretenti st...\n",
      "2    if onli to avoid make thi type of film in the ...\n",
      "3    thi film wa probabl inspir by godard masculin ...\n",
      "4    oh brotheraft hear about thi ridicul film for ...\n",
      "Name: text_stemmed, dtype: object\n",
      "0    i love scifi and am will to put up with a lot ...\n",
      "1    worth the entertain valu of a rental especi if...\n",
      "2    it a total averag film with a few semialright ...\n",
      "3    star rate saturday night friday night friday m...\n",
      "4    first off let me say if you havent enjoy a van...\n",
      "Name: text_stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Define the stemming function\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    # Ensure the input is a string and split it into words\n",
    "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "# Apply stemming to the 'text_tokenized_str' column in the dataset\n",
    "ds_train['text_stemmed'] = ds_train['text_tokenized'].apply(stem_text)\n",
    "ds_test['text_stemmed'] = ds_test['text_tokenized'].apply(stem_text)\n",
    "\n",
    "# Check the first few rows to confirm the stemming\n",
    "print(ds_train['text_stemmed'].head())\n",
    "print(ds_test['text_stemmed'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i rent i be curiousyellow from my video store ...\n",
      "1    i be curious yellow be a risible and pretentio...\n",
      "2    if only to avoid make this type of film in the...\n",
      "3    this film be probably inspire by godards mascu...\n",
      "4    oh brotherafter hear about this ridiculous fil...\n",
      "Name: text_lemmatized, dtype: object\n",
      "0    i love scifi and be will to put up with a lot ...\n",
      "1    worth the entertainment value of a rental espe...\n",
      "2    its a totally average film with a few semialri...\n",
      "3    star rat saturday night friday night friday mo...\n",
      "4    first off let me say if you havent enjoy a van...\n",
      "Name: text_lemmatized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define the lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Ensure the input is a string and split it into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in text.split()]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to the 'text_tokenized_str' column in the dataset\n",
    "ds_train['text_lemmatized'] = ds_train['text_tokenized'].apply(lemmatize_text)\n",
    "ds_test['text_lemmatized'] = ds_test['text_tokenized'].apply(lemmatize_text)\n",
    "\n",
    "# Check the first few rows to confirm the lemmatization\n",
    "print(ds_train['text_lemmatized'].head())\n",
    "print(ds_test['text_lemmatized'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Load NLTK's list of stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Check for rows with only stop words or empty strings\n",
    "# def contains_only_stopwords_or_empty(text):\n",
    "#     # Split the text into words\n",
    "#     words = text.split()\n",
    "#     # Check if all words are stop words or if the text is empty\n",
    "#     return all(word in stop_words for word in words) or len(words) == 0\n",
    "\n",
    "# # Apply the function to the 'text_lemmatized' column\n",
    "# ds_train['only_stopwords_or_empty'] = ds_train['text_lemmatized'].apply(contains_only_stopwords_or_empty)\n",
    "# ds_test['only_stopwords_or_empty'] = ds_test['text_lemmatized'].apply(contains_only_stopwords_or_empty)\n",
    "\n",
    "# # Filter rows where the column contains only stop words or is empty\n",
    "# train_filtered = ds_train[ds_train['only_stopwords_or_empty']]\n",
    "# test_filtered = ds_test[ds_test['only_stopwords_or_empty']]\n",
    "\n",
    "# # Print rows with only stop words or empty strings\n",
    "# print(\"Rows in training set with only stop words or empty strings:\")\n",
    "# print(train_filtered)\n",
    "\n",
    "# print(\"\\nRows in test set with only stop words or empty strings:\")\n",
    "# print(test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Join the tokens in 'text_lemmatized' into a single string for each document\n",
    "ds_train['text_lemmatized'] = ds_train['text_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "ds_test['text_lemmatized'] = ds_test['text_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Fit and transform the 'text_lemmatized_str' column for the training dataset\n",
    "tfidf_train_matrix = tfidf_vectorizer.fit_transform(ds_train['text_lemmatized'])\n",
    "\n",
    "# Transform the 'text_lemmatized_str' column for the test dataset\n",
    "tfidf_test_matrix = tfidf_vectorizer.transform(ds_test['text_lemmatized'])\n",
    "\n",
    "# Convert the training matrix to a DataFrame for better readability (optional)\n",
    "import pandas as pd\n",
    "tfidf_train_df = pd.DataFrame(\n",
    "    tfidf_train_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Convert the test matrix to a DataFrame for better readability (optional)\n",
    "tfidf_test_df = pd.DataFrame(\n",
    "    tfidf_test_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrames (optional)\n",
    "print(\"TF-IDF Training DataFrame:\")\n",
    "print(tfidf_train_df.head())\n",
    "\n",
    "print(\"\\nTF-IDF Test DataFrame:\")\n",
    "print(tfidf_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
