{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizerFast, BertModel\n",
    "from datasets import load, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "dataset['train'] = dataset['train'].select(range(10000))\n",
    "dataset['test'] = dataset['test'].select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into pandas DataFrames\n",
    "X_train = pd.DataFrame(dataset['train'])\n",
    "X_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(X_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the sizes of the splits\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and newlines\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "val_df['text'] = val_df['text'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['label'])\n",
    "val_df['label'] = label_encoder.transform(val_df['label'])\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store model and results\n",
    "output_dir = \"my_model_results\" \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set repo name\n",
    "repo_name = \"Sentiment\"\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    output_dir=repo_name,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# Define or load your model here\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.push_to_hub()\n",
    "\n",
    "\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()  # Saves to the output_dir specified in training_args\n",
    "\n",
    "# Save the tokenizer (optional, but recommended)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
